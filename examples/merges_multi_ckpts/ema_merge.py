import os
import shutil
from transformers import AutoConfig, AutoModelForCausalLM
from tqdm import tqdm
import torch

def merge_checkpoints(checkpoint_dirs, output_dir, model_name="merged_model", alpha=0.05):
    """
    ä½¿ç”¨æŒ‡æ•°å¹³å‡ (EMA) åˆå¹¶å¤šä¸ªLLMæ£€æŸ¥ç‚¹ä¸ºä¸€ä¸ªæ–°çš„æ£€æŸ¥ç‚¹
    
    Args:
        checkpoint_dirs: åŒ…å«å¤šä¸ªæ£€æŸ¥ç‚¹ç›®å½•çš„åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        model_name: åˆå¹¶åçš„æ¨¡å‹åç§°
        alpha: EMA çš„æ›´æ–°ç³»æ•° (é»˜è®¤ 0.05)
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = os.path.join(output_dir, model_name)
    os.makedirs(output_path, exist_ok=True)
    
    # åŠ è½½ç¬¬ä¸€ä¸ªæ£€æŸ¥ç‚¹ä½œä¸ºå‚è€ƒï¼Œè·å–å‚æ•°åå’Œé…ç½®
    ref_checkpoint = checkpoint_dirs[0]
    config = AutoConfig.from_pretrained(ref_checkpoint)
    config.save_pretrained(output_path)

    # å¤åˆ¶åˆ†è¯å™¨ç›¸å…³æ–‡ä»¶
    tokenizer_files = [
        "tokenizer_config.json", "tokenizer.json", "vocab.json",
        "special_tokens_map.json", "model.safetensors.index.json",
        "merges.txt", "added_tokens.json", "config.json", "generation_config.json"
    ]
    for file in tokenizer_files:
        src_path = os.path.join(ref_checkpoint, file)
        if os.path.exists(src_path):
            shutil.copy(src_path, os.path.join(output_path, file))
            
    # åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼ˆå…¨é›¶æƒé‡ï¼‰
    base_model = AutoModelForCausalLM.from_pretrained(ref_checkpoint)
    with torch.no_grad():
        for p in base_model.parameters():
            p.zero_()

    # é€ä¸ª ckpt åš EMA
    with torch.no_grad():
        for ckpt_path in checkpoint_dirs:
            other_model = AutoModelForCausalLM.from_pretrained(ckpt_path)
            for p_base, p_other in tqdm(
                zip(base_model.parameters(), other_model.parameters()),
                total=sum(1 for _ in base_model.parameters()),
                desc=f"EMA åˆå¹¶ {ckpt_path}",
                leave=False
            ):
                p_base.mul_(1 - alpha).add_(p_other, alpha=alpha)
            del other_model
            print(f"âœ… å®Œæˆ EMA æ›´æ–°: {ckpt_path}")


    base_model.save_pretrained(output_path, safe_serialization=True)
    print(f"ğŸ‰ æ‰€æœ‰æ£€æŸ¥ç‚¹å·²ä½¿ç”¨ EMA æˆåŠŸåˆå¹¶ï¼Œä¿å­˜è‡³: {output_path}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    checkpoint_directories = [
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_40/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_50/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_60/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_70/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_80/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_90/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_100/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_110/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_120/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_130/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_140/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_150/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_160/actor_agent_0/huggingface",
        "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/global_step_170/actor_agent_0/huggingface",
    ]
    
    output_directory = "ckpts/Qwen3-8B-Base_cpgd_passk_DAPO-Math-17k_hybrid/nodes4_passk_klCov01coef_train_bsz512_mini_bsz128_rollout_n16_clip_eps_high0.2_lr_warmup_styleconstant_total_epoch10/merage_models/"
    
    # alpha = 2 / (N + 1) = 2 / 12 = 0.16667
    merge_checkpoints(checkpoint_directories, output_directory, "start40_end170_step10_ema-right", alpha=0.15)
